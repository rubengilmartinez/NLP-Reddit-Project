{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1><strong>UNIVERSIDAD POLIT√âCNICA DE CARTAGENA</strong></h1>\n",
        "    <h2><strong>GRADO EN CIENCIA E INGENIER√çA DE DATOS</strong></h2>\n",
        "\n",
        "<div style=\"display:flex; justify-content:center; align-items:center; padding:5px;\">\n",
        "        <img src=\"imgs/upct_logo.png\" style=\"height:300px; width:auto\">\n",
        "    </div>\n",
        "\n",
        "<h2><strong>Procesamiento del Lenguaje Natural</strong></h2>\n",
        "\n",
        "<h3><strong>PROYECTO:</strong><br>\n",
        "    <strong>Aplicaci√≥n PLN para an√°lisis y procesamiento de texto de car√°cter geopol√≠tico, econ√≥mico-burs√°til y pol√≠tico.</strong></h3>\n",
        "\n",
        "<p><strong>ESTUDIANTES</strong></p>\n",
        "    <ul style=\"list-style-type:none; padding: 0;\">\n",
        "        <li>Rub√©n Gil Mart√≠nez</li>\n",
        "        <li>Guillermo L√≥pez P√©rez</li>\n",
        "    </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1) COMPILACI√ìN DEL CORPUS DE COMENTARIOS DE REDDIT A TRAV√âS DE SU API Y USO DE PROCESAMIENTO L√âXICO:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LIBRER√çAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6y8sMCKPXBD4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import datetime as dt\n",
        "import time\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n de credenciales\n",
        "reddit = praw.Reddit(\n",
        "    client_id='v4Ozy7so3ZR5n-i4r7Spqw',\n",
        "    client_secret='iDAtL0KxYY9ky4GyPWyUM2OAZ4LnBg',\n",
        "    user_agent='dl-pln-2025-RGM-GLP'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autenticado como: None\n",
            "T√≠tulo del post de prueba: Some test commands\n",
            "¬°Autenticaci√≥n exitosa!\n"
          ]
        }
      ],
      "source": [
        "# Prueba simple de correcta autenticaci√≥n\n",
        "try:\n",
        "    # Intenta acceder a tu nombre de usuario para verificar autenticaci√≥n\n",
        "    print(f\"Autenticado como: {reddit.user.me()}\")\n",
        "    \n",
        "    # Prueba una solicitud simple\n",
        "    for submission in reddit.subreddit(\"test\").hot(limit=1):\n",
        "        print(f\"T√≠tulo del post de prueba: {submission.title}\")\n",
        "        \n",
        "    print(\"¬°Autenticaci√≥n exitosa!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error de autenticaci√≥n: {type(e).__name__} - {str(e)}\")\n",
        "    print(\"Por favor, verifica tus credenciales y la configuraci√≥n de la aplicaci√≥n.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lista de subreddits a extraer\n",
        "subreddits = ['Economics', 'books', 'travel', 'gaming', 'EnglishLearning', 'Design']\n",
        "\n",
        "# Expresiones regulares para filtrado\n",
        "url_pattern = re.compile(r'(https?://\\S+)')\n",
        "email_pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "\n",
        "# Funci√≥n para verificar si un comentario es v√°lido\n",
        "def is_valid_comment(text):\n",
        "    if len(text) < 30:  # M√≠nimo de 30 caracteres\n",
        "        return False\n",
        "    \n",
        "    # Eliminar URLs y emails\n",
        "    text_cleaned = url_pattern.sub(' ', text)\n",
        "    text_cleaned = email_pattern.sub(' ', text_cleaned)\n",
        "    \n",
        "    # Verificar si queda suficiente contenido\n",
        "    return len(text_cleaned.strip()) >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EXTRACCI√ìN DEL CORPUS DE COMENTARIOS EN ARCHIVOS JSON INDIVIDUALES PARA CADA SUBREDDIT**\n",
        "\n",
        "- 6 subreddits distintos\n",
        "- 20 hilos distintos de cada subreddit\n",
        "- Alrededor de 50 comentarios por cada hilo\n",
        "\n",
        "Durante la extracci√≥n hemos tenido en cuenta el evitar duplicados, tomar comentarios distribuidos en el tiempo (hemos tomado desde comentarios de la √∫ltima semana hasta comentarios de otros a√±os), tomar comentarios que no sean muy cortos y que adem√°s no contengan ni urls ni gmails ya que estos no aportan informaci√≥n relevante. \n",
        "\n",
        "Parte de este filtrado de comentarios ha sido posible gracias a la funci√≥n is_valid_comment() previamente definida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando subreddit: Economics...\n",
            "  Extrayendo de Economics con filtro: all\n",
            "    Procesando hilo: Economist Warns That Elon Musk Is About to Cause a \"Deep, Deep Recession\"\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: 65% of Americans support monthly $2,000 COVID stimulus payments, new poll shows\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Republicans Less Trusted on Economy Than Democrats For First Time in Years\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump's first 50 days mark one of the worst starts for the S&P 500 under any presidency\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: US tourism to suffer huge '¬£49 billion drop' under Donald Trump\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Fed report finds 75% of $800 billion Paycheck Protection Program didn't reach employees\n",
            "      A√±adido con 30 comentarios v√°lidos\n",
            "    Procesando hilo: Trudeau announces Canada ban on Russian oil imports. ‚ÄúToday, we are announcing a ban on all imports of crude oil from Russia, an industry that has benefited President Putin and his oligarchs greatly,‚Äù\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump Admin disbands panels responsible for calculating GDP and collecting economic data\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de Economics con filtro: week\n",
            "    Procesando hilo: ‚ÄòShock to the system‚Äô: farmers hit by Trump‚Äôs tariffs and cuts say they need another bailout\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Foreign investors are dumping U.S. stocks at near-record pace\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump about to trigger greatest trade diversion ever seen\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Fears of the economy tanking are now higher than they were at the height of the Covid pandemic\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Markets Are Discovering the Real Trump Trade Is ‚ÄòSell America‚Äô\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: The Dow plunges 1,000 points and the Nasdaq reels as Trump attacks Powell again\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Americans considering filing for bankruptcy hits highest level since pandemic\n",
            "      A√±adido con 36 comentarios v√°lidos\n",
            "    Procesando hilo: White House Says It Will Seize Wages For Student Loans In Collection‚ÄîHere‚Äôs What Borrowers Can Expect\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de Economics con filtro: month\n",
            "    Procesando hilo: This is the stock market‚Äôs worst start to a presidential term in modern history\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump Is About to Bet the Economy on a Theory That Makes No Sense\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump slaps 104% tariff on China, effective midnight, confirms White House\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Tourism to the US is declining at a rapid pace.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "Completado: Economics - 20 hilos guardados en datasets\\Economics_data_3.0.json\n",
            "\n",
            "Procesando subreddit: books...\n",
            "  Extrayendo de books con filtro: all\n",
            "    Procesando hilo: Join the Battle for Net Neutrality!! We need to stop them from allowing ISPs to charge us extra fees to access ebooks, games or anything else!\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: It's July 29, 2020, the day George R.R. Martin said we could imprison him if Winds Of Winter wasn't done\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Amazon removes books promoting autism cures and vaccine misinformation\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: ‚ÄòReading Rainbow‚Äô Host LeVar Burton Wants to Read to Families for Free, Neil Gaiman Offers Entire Catalog\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Incarcerated Pennsylvanians now have to pay $150 to read. We should all be outraged.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Stephen King briefly talks about the controversial orgy scene in the 'IT' novel. 'It‚Äôs fascinating to me that there has been so much comment about that single sex scene and so little about the multiple child murders. That must mean something, but I‚Äôm not sure what.'\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: In 2018 Jessica Johnson wrote an Orwell prize-winning short story about an algorithm that decides school grades according to social class. This year as a result of the pandemic her A-level English was downgraded by a similar algorithm and she was not accepted for English at St. Andrews University.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Kids get a $2 discount if they read a book aloud to their barber while getting their haircut in Michigan\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de books con filtro: week\n",
            "    Procesando hilo: Roald Dahl's 'The Witches' is a fantastic depiction of a child with a short life expectancy\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Novelist Katie Kitamura: ‚ÄòAs Trump tries to take away everything I love, it‚Äôs never been clearer that writing matters‚Äô\n",
            "      A√±adido con 49 comentarios v√°lidos\n",
            "    Procesando hilo: Author says Naval Academy canceled his lecture over removed book reference\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Do you still remember the hype back then when new Harry Potter books were released? Do you think we could experience this level of hype or something similar in the next 50 years?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Pope Francis reveals some secrets ‚Äî and keeps many others ‚Äî in new memoir\n",
            "      A√±adido con 38 comentarios v√°lidos\n",
            "    Procesando hilo: About the hatred for Holden Caulfield...\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Finished reading Name of the Rose\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: The Parable of the Talents: the best book I never want to read again.\n",
            "      A√±adido con 26 comentarios v√°lidos\n",
            "  Extrayendo de books con filtro: month\n",
            "    Procesando hilo: George R.R. Martin says 'The Winds of Winter' is 'the curse of my life'\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Trump administration reportedly moves to ban Jackie Robinson biography from Naval Academy library\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: This is how Facebook won Donald Trump the 2016 election.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Entire staff at federal agency that funds libraries and museums put on leave\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "Completado: books - 20 hilos guardados en datasets\\books_data_3.0.json\n",
            "\n",
            "Procesando subreddit: travel...\n",
            "  Extrayendo de travel con filtro: all\n",
            "    Procesando hilo: I cycled across Mongolia. Here are some pics. \n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I visited North Korea recently, these are some of the photos.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: i went on a solo trip thinking i‚Äôd ‚Äúfind myself‚Äù or whatever‚Ä¶ but instead i got drunk w a 73yo italian grandma & ended up in a village wedding??\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Taken with a phone out of my hotel window in Venice... and no boats!\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Taking a ride on the Bernina Express through the Alps\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Wife and I hate big social events and love traveling. So rather than a normal wedding, traveled to Switzerland and did our vows in private. Photo from the day\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: The exact moment I took a step too close to the border between North and South Korea and got a push on the butt from two NK soldiers. What's your favorite travel photo of yourself?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Splurged on a hotel in Patagonia\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de travel con filtro: week\n",
            "    Procesando hilo: Tokyo under cherry blossoms, Mar/Apr 2025\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I keep getting stopped at the US border and I just found it‚Äôs because of multiple ‚Äúno shows‚Äù?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Four days in Tirana, Albania in April 2025\n",
            "      A√±adido con 46 comentarios v√°lidos\n",
            "    Procesando hilo: Istanbul has gone over the edge as an enjoyable vacation destination.  It is legitimately nothing more than a nuisance now\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Is it just me or are airports weird?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: how do people have the means to travel regularly\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Unpopular Opinion: I'd rather sit at home than an airport lounge\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Do border guards worldwide have a secret competition who puts more crooked stamps in most random passport pages?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de travel con filtro: month\n",
            "    Procesando hilo: First and Maybe Last Visit to India?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Oaxaca, Mexico ‚Äî If there was ever a city in Mexico that embodies the spirit of magical realism this is it\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: My trip to Antarctica: icebergs, penguins, sleeping on the ice etc.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Passport was taken away when coming home from international flight?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "Completado: travel - 20 hilos guardados en datasets\\travel_data_3.0.json\n",
            "\n",
            "Procesando subreddit: gaming...\n",
            "  Extrayendo de gaming con filtro: all\n",
            "    Procesando hilo: Take your time, you got this\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: My parents (late 70s) got me a ps5 controller for Christmas. I do not own a playstation 5...\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I got off the horse by accident right before a cutscene in red dead\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: boy gamer\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Minesweeper 99 x 99, 1500 mines. Took me about 2.5 hours to finish, nerve-wracking. No one might care, but just wanted to share this.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: The perfect cosplay doesn‚Äôt ex...\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: 'Play until we lose'\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Can we please boycott Star Wars battlefront 2\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de gaming con filtro: week\n",
            "    Procesando hilo: Not a single NPC has thanked me once.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Bethesda has gifted every member of Skyblivion Team free keys for Oblivion Remastered, following its release earlier today.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I just bought red dead redemption, feeling excited!\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: The Oblivion remaster character creator is just insane.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I‚Äôve defeated the final boss of Elden ring 10,000 times as a summon! My characters name is LetMeSoloThem (let me solo her inspired me, but we are not the same person)\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Maxing sneak in Oblivion the proper way\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Is this what it feels like to play a game for the first time again?(oblivion)\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Traded a few things for a Lenovo legion go today\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de gaming con filtro: month\n",
            "    Procesando hilo: A comparison between the most graphically detailed eyes in gaming\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: My wife thinks video games are juvenile and playing them makes me less attractive.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Game console button layout\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de gaming con filtro: year\n",
            "    Procesando hilo: The Nostalgia\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "Completado: gaming - 20 hilos guardados en datasets\\gaming_data_3.0.json\n",
            "\n",
            "Procesando subreddit: EnglishLearning...\n",
            "  Extrayendo de EnglishLearning con filtro: all\n",
            "    Procesando hilo: American English vs British English \n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why is there an apostrophe?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Well.....nobody is perfect :)\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: What does it mean?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: What actually means draw in this context?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: English is definitely a weird language.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: What does this mean? \n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why is it singular?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de EnglishLearning con filtro: week\n",
            "    Procesando hilo: Which one is it?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: what does ‚Äòthe fuck out of me‚Äô mean?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: How can I speak respectfully in English without using honorifics like 'Anh', 'Ch·ªã', or 'Ch√∫'?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Can someomne explain to me why the To in the frist sentence, please?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why is there a \"to\" in the last sentence? It sounds more natural to me without it\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Do average English natives know this word?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Help me with this question\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why is this question considered ‚Äòawful English‚Äô?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de EnglishLearning con filtro: month\n",
            "    Procesando hilo: It ain't easy\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why can't I say nobody instead of no one?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Hi native speakers, would you say this is a difficult test?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: How do I retain my English level?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "Completado: EnglishLearning - 20 hilos guardados en datasets\\EnglishLearning_data_3.0.json\n",
            "\n",
            "Procesando subreddit: Design...\n",
            "  Extrayendo de Design con filtro: all\n",
            "    Procesando hilo: The new cover of TIME\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Take only what you need\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: National Geographic Cover\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: I saw this logo today, I think it's a fantastic design\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: My 5-minute redesign of Hillary's new logo. What do you think?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: What clients really want.\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: üçä\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Some of my recent obsession with red/orange\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de Design con filtro: week\n",
            "    Procesando hilo: Which one is better?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Losing Income to AI\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Woodstock Inn Brewery using AI on their beer cans\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: We are not a company that needs a designer...\n",
            "      A√±adido con 44 comentarios v√°lidos\n",
            "    Procesando hilo: A very simple Halftone generator tool\n",
            "      Descartado: solo 12 comentarios v√°lidos\n",
            "    Procesando hilo: Adobe? Are you really playing f*king videos when I open PhotoShop?! OMFG.\n",
            "      A√±adido con 40 comentarios v√°lidos\n",
            "    Procesando hilo: Is this how you vertically alligning numbers?\n",
            "      A√±adido con 26 comentarios v√°lidos\n",
            "    Procesando hilo: Can't Find a Job... Am I Being Aged Out?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "  Extrayendo de Design con filtro: month\n",
            "    Procesando hilo: Am I the only one who isn‚Äôt bothered by this?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why do consumers hate it when brands try to connect with them?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Is this the end?\n",
            "      A√±adido con 50 comentarios v√°lidos\n",
            "    Procesando hilo: Why do AI company logos look like buttholes?\n",
            "      A√±adido con 41 comentarios v√°lidos\n",
            "    Procesando hilo: Who else wants Disney to bring back 2D animation?\n",
            "      A√±adido con 40 comentarios v√°lidos\n",
            "Completado: Design - 20 hilos guardados en datasets\\Design_data_3.0.json\n",
            "\n",
            "Extracci√≥n completa.\n"
          ]
        }
      ],
      "source": [
        "# Procesamiento de cada subreddit\n",
        "for subreddit_name in subreddits:\n",
        "    print(f\"Procesando subreddit: {subreddit_name}...\")\n",
        "    \n",
        "    subreddit_data = {\n",
        "        \"subreddit_name\": subreddit_name,\n",
        "        \"threads\": []\n",
        "    }\n",
        "\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    threads_collected = 0  # Contador de hilos extra√≠dos\n",
        "    extracted_thread_ids = set()  # Para evitar duplicados\n",
        "\n",
        "    # Diferentes filtros de tiempo\n",
        "    time_filters = ['all', 'week', 'month', 'year']\n",
        "\n",
        "    for time_filter in time_filters:\n",
        "        if threads_collected >= 20:\n",
        "            break\n",
        "        \n",
        "        print(f\"  Extrayendo de {subreddit_name} con filtro: {time_filter}\")\n",
        "\n",
        "        try:\n",
        "            threads = list(subreddit.top(time_filter=time_filter, limit=8))  # Procesamos 8 hilos para cada temporalidad para mayor rapidez y evitar bloqueos\n",
        "            \n",
        "            for thread in threads:\n",
        "                if threads_collected >= 20 or thread.id in extracted_thread_ids:  # Evitar duplicados\n",
        "                    continue  \n",
        "\n",
        "                print(f\"    Procesando hilo: {thread.title}\")\n",
        "\n",
        "                # Informaci√≥n del hilo\n",
        "                thread_info = {\n",
        "                    \"thread_id\": thread.id,\n",
        "                    \"title\": thread.title,\n",
        "                    \"author\": str(thread.author),\n",
        "                    \"created_utc\": datetime.utcfromtimestamp(thread.created_utc).isoformat(),\n",
        "                    \"score\": thread.score,\n",
        "                    \"url\": thread.url,\n",
        "                    \"permalink\": thread.permalink,\n",
        "                    \"time_filter\": time_filter,\n",
        "                    \"subreddit_name\": subreddit_name,\n",
        "                    \"comments\": []\n",
        "                }\n",
        "\n",
        "\n",
        "                # Extraer comentarios\n",
        "                try:\n",
        "                    thread.comments.replace_more(limit=3)  # Evita cargar demasiados comentarios\n",
        "                    flat_comments = thread.comments.list()\n",
        "                except Exception as e:\n",
        "                    print(f\"      Error al cargar comentarios: {e}\")\n",
        "                    continue\n",
        "\n",
        "                valid_comments_count = 0\n",
        "\n",
        "                for comment in flat_comments:\n",
        "                    if valid_comments_count >= 50:\n",
        "                        break\n",
        "\n",
        "                    if is_valid_comment(comment.body): # Si el comentario pasa el filtro de validez es almacenado en la lista de comentarios de su respectivo hilo\n",
        "\n",
        "                        comment_info = {\n",
        "                            'comment_id': comment.id,\n",
        "                            'author': str(comment.author),\n",
        "                            'text': comment.body,\n",
        "                            'score': comment.score,\n",
        "                            'created_utc': datetime.utcfromtimestamp(comment.created_utc).isoformat(),\n",
        "                            'subreddit_name': subreddit_name,\n",
        "                            'thread_id': thread.id,\n",
        "                            'is_submitter': comment.is_submitter,\n",
        "                            'permalink': comment.permalink\n",
        "                        }\n",
        "\n",
        "                        thread_info[\"comments\"].append(comment_info)\n",
        "                        valid_comments_count += 1\n",
        "\n",
        "                if valid_comments_count >= 25:  # M√≠nimo de 25 comentarios v√°lidos, sino, el hilo es descartado\n",
        "                    subreddit_data[\"threads\"].append(thread_info)\n",
        "                    extracted_thread_ids.add(thread.id)\n",
        "                    threads_collected += 1\n",
        "                    print(f\"      A√±adido con {valid_comments_count} comentarios v√°lidos\")\n",
        "                else:\n",
        "                    print(f\"      Descartado: solo {valid_comments_count} comentarios v√°lidos\")\n",
        "\n",
        "                time.sleep(1.5)  # Evitar exceso de peticiones a Reddit, nos permite cumplir con las normas de uso de la API y evitar bloqueos temporales\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {subreddit_name} con filtro {time_filter}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Guardar en JSON\n",
        "    os.makedirs('datasets', exist_ok=True)\n",
        "    output_file = os.path.join('datasets', f\"{subreddit_name}_data_3.0.json\")\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(subreddit_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Completado: {subreddit_name} - {threads_collected} hilos guardados en {output_file}\\n\")\n",
        "\n",
        "print(\"Extracci√≥n completa.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PROCESAMIENTO L√âXICO Y CREACI√ìN DE UN DATAFRAME PARA TRATAR CON MAYOR FACILIDAD LOS COMENTARIOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "import spacy\n",
        "\n",
        "# Cargamos el modelo de spaCy para realizar la Lematizaci√≥n\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Configuraci√≥n de SymSpell para realizar la correcci√≥n autom√°tica de errores ortogr√°ficos\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = \"datasets/frequency_dictionary_en_82_765.txt\"  \n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1, separator=\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    # Patr√≥n para detectar emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos & pictogramas\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # s√≠mbolos de transporte & mapas\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FUNCI√ìN DE PROCESAMIENTO L√âXICO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, autocorrect=True):\n",
        "    # Eliminar emojis\n",
        "    text = remove_emojis(text)\n",
        "    \n",
        "    # Convertimos a min√∫sculas\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Procesamiento con spaCy\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Tokenizaci√≥n y eliminaci√≥n de signos de puntuaci√≥n y n√∫meros\n",
        "    tokens = [token.text for token in doc if not token.is_punct and not token.like_num]\n",
        "    \n",
        "    # Correcci√≥n ortogr√°fica con SymSpellPy\n",
        "    if autocorrect:\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            suggestions = sym_spell.lookup(token, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "            # Si hay sugerencias, tomamos la primera\n",
        "            # Si no, mantenemos el token original\n",
        "            if suggestions:\n",
        "                corrected_tokens.append(suggestions[0].term)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        tokens = corrected_tokens\n",
        "    \n",
        "    # Eliminaci√≥n de stopwords\n",
        "    tokens = [token for token in tokens if not token in nlp.Defaults.stop_words]\n",
        "    \n",
        "    # Lematizaci√≥n con spaCy\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "    \n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Procesamiento L√©xico con spaCy y SymSpellPy\n",
        "\n",
        "Usamos **spaCy** para el procesamiento completo del texto. Aprovechamos sus capacidades integradas para:\n",
        "\n",
        "- Tokenizaci√≥n  \n",
        "- Detecci√≥n de n√∫meros y puntuaci√≥n  \n",
        "- Uso de su propia lista de *stopwords*  \n",
        "- Lematizaci√≥n m√°s precisa  \n",
        "\n",
        "Adem√°s, realizamos la **correcci√≥n ortogr√°fica** con **SymSpellPy**.\n",
        "\n",
        "### Ventajas de usar spaCy\n",
        "\n",
        "El proceso es m√°s eficiente porque spaCy est√° optimizado para procesamiento de texto. La lematizaci√≥n con spaCy suele ser m√°s precisa que con NLTK porque:\n",
        "\n",
        "- Considera el contexto de la palabra  \n",
        "- Tiene mejor manejo de formas irregulares  \n",
        "- Est√° m√°s actualizado con el lenguaje moderno  \n",
        "- Es m√°s r√°pido en el procesamiento  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**REALIZACI√ìN DEL PROCESAMIENTO Y PREPARACI√ìN DE LOS COMENTARIOS PARA SU POSTERIOR USO EN DIVERSAS TAREAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos los datos extra√≠dos y procesamos los comentarios\n",
        "all_comments = []\n",
        "dataset_path = 'datasets'\n",
        "\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith('_data_3.0.json'):\n",
        "        with open(os.path.join(dataset_path, filename), 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "            \n",
        "            for thread in data['threads']:\n",
        "                for comment in thread['comments']:\n",
        "                    comment_data = {\n",
        "                        'body': comment['text'],\n",
        "                        'score': comment['score'],\n",
        "                        'created_utc': comment['created_utc'],\n",
        "                        'thread_title': thread['title'],\n",
        "                        'subreddit': data['subreddit_name'],\n",
        "                        'processed_text': preprocess_text(comment['text'], autocorrect=True),\n",
        "                        'processed_text_no_correction': preprocess_text(comment['text'], autocorrect=False)\n",
        "                    }\n",
        "                    all_comments.append(comment_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CREACI√ìN DE UN DATAFRAME CON TODOS LOS COMENTARIOS EXTRA√çDOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5866\n"
          ]
        }
      ],
      "source": [
        "print(len(all_comments))  # Obtenci√≥n de alrededor de 6000 comentarios v√°lidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_text_no_correction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So if you haven't already, there's a bot you c...</td>\n",
              "      <td>2472</td>\n",
              "      <td>2017-11-22T01:50:57</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aussie here. Please fight for this. If the US ...</td>\n",
              "      <td>697</td>\n",
              "      <td>2017-11-22T03:19:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do we have to do this every few months? üòû</td>\n",
              "      <td>810</td>\n",
              "      <td>2017-11-22T03:17:01</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>month</td>\n",
              "      <td>month</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>üí∞ The üí∞ intent üí∞ is üí∞ to üí∞ provide üí∞ consumers...</td>\n",
              "      <td>871</td>\n",
              "      <td>2017-11-22T02:21:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>intent        provide    consumer        ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Someone suggested that we flood Trumps twitter...</td>\n",
              "      <td>124</td>\n",
              "      <td>2017-11-22T04:41:47</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>suggest flood trump twitter tweet save net neu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Fucking same shit every month. How many times ...</td>\n",
              "      <td>72</td>\n",
              "      <td>2017-11-22T03:25:33</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>fucking shit month time go try pass shit</td>\n",
              "      <td>fucking shit month time go try pass shit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>We've only done 266k calls... how? We have mor...</td>\n",
              "      <td>261</td>\n",
              "      <td>2017-11-22T03:13:33</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>ave 266k call editor upcoming caller literally</td>\n",
              "      <td>266k call reditor upvote caller literally</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>‚ÄúThe intent is to provide customers with a sen...</td>\n",
              "      <td>118</td>\n",
              "      <td>2017-11-22T03:29:12</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide customer sense accomplishment u...</td>\n",
              "      <td>intent provide customer sense accomplishment u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[To learn about Net Neutrality, why it's impor...</td>\n",
              "      <td>99</td>\n",
              "      <td>2017-11-22T03:13:49</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>learn net neutrality important indoor want too...</td>\n",
              "      <td>learn net neutrality important and/or want too...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Well y'all, I sat through all the prompts and ...</td>\n",
              "      <td>24</td>\n",
              "      <td>2017-11-22T03:36:15</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>sit prompt leave message senator representativ...</td>\n",
              "      <td>y ' sit prompt leave message senator represent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body  score  \\\n",
              "0  So if you haven't already, there's a bot you c...   2472   \n",
              "1  Aussie here. Please fight for this. If the US ...    697   \n",
              "2      Why do we have to do this every few months? üòû    810   \n",
              "3  üí∞ The üí∞ intent üí∞ is üí∞ to üí∞ provide üí∞ consumers...    871   \n",
              "4  Someone suggested that we flood Trumps twitter...    124   \n",
              "5  Fucking same shit every month. How many times ...     72   \n",
              "6  We've only done 266k calls... how? We have mor...    261   \n",
              "7  ‚ÄúThe intent is to provide customers with a sen...    118   \n",
              "8  [To learn about Net Neutrality, why it's impor...     99   \n",
              "9  Well y'all, I sat through all the prompts and ...     24   \n",
              "\n",
              "           created_utc                                       thread_title  \\\n",
              "0  2017-11-22T01:50:57  Join the Battle for Net Neutrality!! We need t...   \n",
              "1  2017-11-22T03:19:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "2  2017-11-22T03:17:01  Join the Battle for Net Neutrality!! We need t...   \n",
              "3  2017-11-22T02:21:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "4  2017-11-22T04:41:47  Join the Battle for Net Neutrality!! We need t...   \n",
              "5  2017-11-22T03:25:33  Join the Battle for Net Neutrality!! We need t...   \n",
              "6  2017-11-22T03:13:33  Join the Battle for Net Neutrality!! We need t...   \n",
              "7  2017-11-22T03:29:12  Join the Battle for Net Neutrality!! We need t...   \n",
              "8  2017-11-22T03:13:49  Join the Battle for Net Neutrality!! We need t...   \n",
              "9  2017-11-22T03:36:15  Join the Battle for Net Neutrality!! We need t...   \n",
              "\n",
              "  subreddit                                     processed_text  \\\n",
              "0     books  bot text help write email fax free charge sena...   \n",
              "1     books          aussie fight lose net neutrality overflow   \n",
              "2     books                                              month   \n",
              "3     books  intent provide consumer high quality service s...   \n",
              "4     books  suggest flood trump twitter sweet save net neu...   \n",
              "5     books           fucking shit month time go try pass shit   \n",
              "6     books     ave 266k call editor upcoming caller literally   \n",
              "7     books  intent provide customer sense accomplishment u...   \n",
              "8     books  learn net neutrality important indoor want too...   \n",
              "9     books  sit prompt leave message senator representativ...   \n",
              "\n",
              "                        processed_text_no_correction  \n",
              "0  bot text help write email fax free charge sena...  \n",
              "1          aussie fight lose net neutrality overflow  \n",
              "2                                              month  \n",
              "3       intent        provide    consumer        ...  \n",
              "4  suggest flood trump twitter tweet save net neu...  \n",
              "5           fucking shit month time go try pass shit  \n",
              "6          266k call reditor upvote caller literally  \n",
              "7  intent provide customer sense accomplishment u...  \n",
              "8  learn net neutrality important and/or want too...  \n",
              "9  y ' sit prompt leave message senator represent...  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Crear DataFrame\n",
        "df_comments = pd.DataFrame(all_comments)\n",
        "\n",
        "# Mostrar las primeras filas y informaci√≥n del DataFrame\n",
        "df_comments.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_comments.to_csv('datasets/comments_data.csv', index=False, header=True, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")  # Tambi√©n hay 50, 200 o 300 dimensiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_text</th>\n",
              "      <th>embedding_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>[-0.11024348, 0.13865672, 0.24223563, -0.19040...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>[0.27225667, 0.19225435, 0.43482652, -0.350442...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>month</td>\n",
              "      <td>[0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>[-0.05243463, 0.061616946, 0.26537868, 0.02395...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>[-0.05058644, -0.0106403595, 0.18712802, -0.19...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      processed_text  \\\n",
              "0  bot text help write email fax free charge sena...   \n",
              "1          aussie fight lose net neutrality overflow   \n",
              "2                                              month   \n",
              "3  intent provide consumer high quality service s...   \n",
              "4  suggest flood trump twitter sweet save net neu...   \n",
              "\n",
              "                                      embedding_mean  \n",
              "0  [-0.11024348, 0.13865672, 0.24223563, -0.19040...  \n",
              "1  [0.27225667, 0.19225435, 0.43482652, -0.350442...  \n",
              "2  [0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...  \n",
              "3  [-0.05243463, 0.061616946, 0.26537868, 0.02395...  \n",
              "4  [-0.05058644, -0.0106403595, 0.18712802, -0.19...  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Funci√≥n para obtener el promedio de embeddings de las palabras del texto\n",
        "def get_mean_embedding(text):\n",
        "    vectors = [model[word] for word in text.split() if word in model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.nan  # o np.zeros(50) si prefieres un vector nulo\n",
        "\n",
        "# Aplicamos la funci√≥n a la columna del DataFrame\n",
        "df_comments[\"embedding_mean\"] = df_comments[\"processed_text\"].apply(get_mean_embedding)\n",
        "\n",
        "# Mostramos los primeros resultados\n",
        "df_comments[[\"processed_text\", \"embedding_mean\"]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_text_no_correction</th>\n",
              "      <th>embedding_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So if you haven't already, there's a bot you c...</td>\n",
              "      <td>2472</td>\n",
              "      <td>2017-11-22T01:50:57</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>[-0.11024348, 0.13865672, 0.24223563, -0.19040...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aussie here. Please fight for this. If the US ...</td>\n",
              "      <td>697</td>\n",
              "      <td>2017-11-22T03:19:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>[0.27225667, 0.19225435, 0.43482652, -0.350442...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do we have to do this every few months? üòû</td>\n",
              "      <td>810</td>\n",
              "      <td>2017-11-22T03:17:01</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>month</td>\n",
              "      <td>month</td>\n",
              "      <td>[0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>üí∞ The üí∞ intent üí∞ is üí∞ to üí∞ provide üí∞ consumers...</td>\n",
              "      <td>871</td>\n",
              "      <td>2017-11-22T02:21:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>intent        provide    consumer        ...</td>\n",
              "      <td>[-0.05243463, 0.061616946, 0.26537868, 0.02395...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Someone suggested that we flood Trumps twitter...</td>\n",
              "      <td>124</td>\n",
              "      <td>2017-11-22T04:41:47</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>suggest flood trump twitter tweet save net neu...</td>\n",
              "      <td>[-0.05058644, -0.0106403595, 0.18712802, -0.19...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body  score  \\\n",
              "0  So if you haven't already, there's a bot you c...   2472   \n",
              "1  Aussie here. Please fight for this. If the US ...    697   \n",
              "2      Why do we have to do this every few months? üòû    810   \n",
              "3  üí∞ The üí∞ intent üí∞ is üí∞ to üí∞ provide üí∞ consumers...    871   \n",
              "4  Someone suggested that we flood Trumps twitter...    124   \n",
              "\n",
              "           created_utc                                       thread_title  \\\n",
              "0  2017-11-22T01:50:57  Join the Battle for Net Neutrality!! We need t...   \n",
              "1  2017-11-22T03:19:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "2  2017-11-22T03:17:01  Join the Battle for Net Neutrality!! We need t...   \n",
              "3  2017-11-22T02:21:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "4  2017-11-22T04:41:47  Join the Battle for Net Neutrality!! We need t...   \n",
              "\n",
              "  subreddit                                     processed_text  \\\n",
              "0     books  bot text help write email fax free charge sena...   \n",
              "1     books          aussie fight lose net neutrality overflow   \n",
              "2     books                                              month   \n",
              "3     books  intent provide consumer high quality service s...   \n",
              "4     books  suggest flood trump twitter sweet save net neu...   \n",
              "\n",
              "                        processed_text_no_correction  \\\n",
              "0  bot text help write email fax free charge sena...   \n",
              "1          aussie fight lose net neutrality overflow   \n",
              "2                                              month   \n",
              "3       intent        provide    consumer        ...   \n",
              "4  suggest flood trump twitter tweet save net neu...   \n",
              "\n",
              "                                      embedding_mean  \n",
              "0  [-0.11024348, 0.13865672, 0.24223563, -0.19040...  \n",
              "1  [0.27225667, 0.19225435, 0.43482652, -0.350442...  \n",
              "2  [0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...  \n",
              "3  [-0.05243463, 0.061616946, 0.26537868, 0.02395...  \n",
              "4  [-0.05058644, -0.0106403595, 0.18712802, -0.19...  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_comments.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Design       0.62      0.53      0.57       188\n",
            "      Economics       0.69      0.83      0.76       193\n",
            "EnglishLearning       0.76      0.65      0.70       198\n",
            "          books       0.60      0.54      0.57       193\n",
            "         gaming       0.58      0.62      0.60       200\n",
            "         travel       0.63      0.68      0.65       199\n",
            "\n",
            "       accuracy                           0.64      1171\n",
            "      macro avg       0.65      0.64      0.64      1171\n",
            "   weighted avg       0.65      0.64      0.64      1171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convertir las listas de 'embedding_padded' en una matriz de NumPy\n",
        "X = np.vstack(df_comments[\"embedding_mean\"].dropna().values)\n",
        "\n",
        "# Etiquetas\n",
        "y = df_comments.loc[df_comments[\"embedding_mean\"].notna(), \"subreddit\"] # Aseg√∫rate de que las etiquetas est√©n alineadas con X\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Crear el clasificador Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "# Entrenar el modelo\n",
        "rf.fit(X_train, y_train)\n",
        "# Hacer predicciones\n",
        "y_pred = rf.predict(X_test)\n",
        "# Evaluar el modelo\n",
        "print(classification_report(y_test, y_pred, zero_division=0))  # A√±adido zero_division=0 para evitar errores en caso de divisi√≥n por cero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
            "Mejores par√°metros encontrados: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "\n",
            "Reporte en test set:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Design       0.61      0.64      0.62       188\n",
            "      Economics       0.72      0.85      0.78       193\n",
            "EnglishLearning       0.76      0.71      0.73       198\n",
            "          books       0.68      0.60      0.63       193\n",
            "         gaming       0.67      0.63      0.65       200\n",
            "         travel       0.70      0.72      0.71       199\n",
            "\n",
            "       accuracy                           0.69      1171\n",
            "      macro avg       0.69      0.69      0.69      1171\n",
            "   weighted avg       0.69      0.69      0.69      1171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Definir el espacio de b√∫squeda de hiperpar√°metros\n",
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['linear'],\n",
        "        'C': [0.1, 1, 10]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['rbf'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': ['scale', 0.01, 0.1, 1]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'degree': [2, 3, 4],\n",
        "        'gamma': ['scale', 0.01, 0.1]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Crear el GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs=-1  # Usa todos los n√∫cleos disponibles\n",
        ")\n",
        "\n",
        "# Ajustar al set de entrenamiento\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mostrar los mejores par√°metros\n",
        "print(\"Mejores par√°metros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluar el mejor modelo en el set de prueba\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nReporte en test set:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM7WT3JKuGij/JurAfV1xur",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mi_entorno",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
