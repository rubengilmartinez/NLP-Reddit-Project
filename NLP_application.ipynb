{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1><strong>UNIVERSIDAD POLITÉCNICA DE CARTAGENA</strong></h1>\n",
        "    <h2><strong>GRADO EN CIENCIA E INGENIERÍA DE DATOS</strong></h2>\n",
        "\n",
        "<div style=\"display:flex; justify-content:center; align-items:center; padding:5px;\">\n",
        "        <img src=\"imgs/upct_logo.png\" style=\"height:300px; width:auto\">\n",
        "    </div>\n",
        "\n",
        "<h2><strong>Procesamiento del Lenguaje Natural</strong></h2>\n",
        "\n",
        "<h3><strong>PROYECTO:</strong><br>\n",
        "    <strong>Aplicación PLN para análisis y procesamiento de texto de carácter geopolítico, económico-bursátil y político.</strong></h3>\n",
        "\n",
        "<p><strong>ESTUDIANTES</strong></p>\n",
        "    <ul style=\"list-style-type:none; padding: 0;\">\n",
        "        <li>Rubén Gil Martínez</li>\n",
        "        <li>Guillermo López Pérez</li>\n",
        "    </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1) COMPILACIÓN DEL CORPUS DE COMENTARIOS DE REDDIT A TRAVÉS DE SU API Y USO DE PROCESAMIENTO LÉXICO:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LIBRERÍAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6y8sMCKPXBD4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import datetime as dt\n",
        "import time\n",
        "\n",
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de credenciales\n",
        "reddit = praw.Reddit(\n",
        "    client_id='v4Ozy7so3ZR5n-i4r7Spqw',\n",
        "    client_secret='iDAtL0KxYY9ky4GyPWyUM2OAZ4LnBg',\n",
        "    user_agent='dl-pln-2025-RGM-GLP'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autenticado como: None\n",
            "Título del post de prueba: Some test commands\n",
            "¡Autenticación exitosa!\n"
          ]
        }
      ],
      "source": [
        "# Prueba simple de correcta autenticación\n",
        "try:\n",
        "    # Intenta acceder a tu nombre de usuario para verificar autenticación\n",
        "    print(f\"Autenticado como: {reddit.user.me()}\")\n",
        "    \n",
        "    # Prueba una solicitud simple\n",
        "    for submission in reddit.subreddit(\"test\").hot(limit=1):\n",
        "        print(f\"Título del post de prueba: {submission.title}\")\n",
        "        \n",
        "    print(\"¡Autenticación exitosa!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error de autenticación: {type(e).__name__} - {str(e)}\")\n",
        "    print(\"Por favor, verifica tus credenciales y la configuración de la aplicación.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lista de subreddits a extraer\n",
        "subreddits = ['Economics', 'books', 'travel', 'gaming', 'EnglishLearning', 'Design']\n",
        "\n",
        "# Expresiones regulares para filtrado\n",
        "url_pattern = re.compile(r'(https?://\\S+)')\n",
        "email_pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "\n",
        "# Función para verificar si un comentario es válido\n",
        "def is_valid_comment(text):\n",
        "    if len(text) < 30:  # Mínimo de 30 caracteres\n",
        "        return False\n",
        "    \n",
        "    # Eliminar URLs y emails\n",
        "    text_cleaned = url_pattern.sub(' ', text)\n",
        "    text_cleaned = email_pattern.sub(' ', text_cleaned)\n",
        "    \n",
        "    # Verificar si queda suficiente contenido\n",
        "    return len(text_cleaned.strip()) >= 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EXTRACCIÓN DEL CORPUS DE COMENTARIOS EN ARCHIVOS JSON INDIVIDUALES PARA CADA SUBREDDIT**\n",
        "\n",
        "- 6 subreddits distintos\n",
        "- 20 hilos distintos de cada subreddit\n",
        "- Alrededor de 50 comentarios por cada hilo\n",
        "\n",
        "Durante la extracción hemos tenido en cuenta el evitar duplicados, tomar comentarios distribuidos en el tiempo (hemos tomado desde comentarios de la última semana hasta comentarios de otros años), tomar comentarios que no sean muy cortos y que además no contengan ni urls ni gmails ya que estos no aportan información relevante. \n",
        "\n",
        "Parte de este filtrado de comentarios ha sido posible gracias a la función is_valid_comment() previamente definida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando subreddit: Economics...\n",
            "  Extrayendo de Economics con filtro: all\n",
            "    Procesando hilo: Economist Warns That Elon Musk Is About to Cause a \"Deep, Deep Recession\"\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: 65% of Americans support monthly $2,000 COVID stimulus payments, new poll shows\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Republicans Less Trusted on Economy Than Democrats For First Time in Years\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump's first 50 days mark one of the worst starts for the S&P 500 under any presidency\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: US tourism to suffer huge '£49 billion drop' under Donald Trump\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Fed report finds 75% of $800 billion Paycheck Protection Program didn't reach employees\n",
            "      Añadido con 30 comentarios válidos\n",
            "    Procesando hilo: Trudeau announces Canada ban on Russian oil imports. “Today, we are announcing a ban on all imports of crude oil from Russia, an industry that has benefited President Putin and his oligarchs greatly,”\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump Admin disbands panels responsible for calculating GDP and collecting economic data\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de Economics con filtro: week\n",
            "    Procesando hilo: ‘Shock to the system’: farmers hit by Trump’s tariffs and cuts say they need another bailout\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Foreign investors are dumping U.S. stocks at near-record pace\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump about to trigger greatest trade diversion ever seen\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Fears of the economy tanking are now higher than they were at the height of the Covid pandemic\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Markets Are Discovering the Real Trump Trade Is ‘Sell America’\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: The Dow plunges 1,000 points and the Nasdaq reels as Trump attacks Powell again\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Americans considering filing for bankruptcy hits highest level since pandemic\n",
            "      Añadido con 36 comentarios válidos\n",
            "    Procesando hilo: White House Says It Will Seize Wages For Student Loans In Collection—Here’s What Borrowers Can Expect\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de Economics con filtro: month\n",
            "    Procesando hilo: This is the stock market’s worst start to a presidential term in modern history\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump Is About to Bet the Economy on a Theory That Makes No Sense\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump slaps 104% tariff on China, effective midnight, confirms White House\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Tourism to the US is declining at a rapid pace.\n",
            "      Añadido con 50 comentarios válidos\n",
            "Completado: Economics - 20 hilos guardados en datasets\\Economics_data_3.0.json\n",
            "\n",
            "Procesando subreddit: books...\n",
            "  Extrayendo de books con filtro: all\n",
            "    Procesando hilo: Join the Battle for Net Neutrality!! We need to stop them from allowing ISPs to charge us extra fees to access ebooks, games or anything else!\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: It's July 29, 2020, the day George R.R. Martin said we could imprison him if Winds Of Winter wasn't done\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Amazon removes books promoting autism cures and vaccine misinformation\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: ‘Reading Rainbow’ Host LeVar Burton Wants to Read to Families for Free, Neil Gaiman Offers Entire Catalog\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Incarcerated Pennsylvanians now have to pay $150 to read. We should all be outraged.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Stephen King briefly talks about the controversial orgy scene in the 'IT' novel. 'It’s fascinating to me that there has been so much comment about that single sex scene and so little about the multiple child murders. That must mean something, but I’m not sure what.'\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: In 2018 Jessica Johnson wrote an Orwell prize-winning short story about an algorithm that decides school grades according to social class. This year as a result of the pandemic her A-level English was downgraded by a similar algorithm and she was not accepted for English at St. Andrews University.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Kids get a $2 discount if they read a book aloud to their barber while getting their haircut in Michigan\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de books con filtro: week\n",
            "    Procesando hilo: Roald Dahl's 'The Witches' is a fantastic depiction of a child with a short life expectancy\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Novelist Katie Kitamura: ‘As Trump tries to take away everything I love, it’s never been clearer that writing matters’\n",
            "      Añadido con 49 comentarios válidos\n",
            "    Procesando hilo: Author says Naval Academy canceled his lecture over removed book reference\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Do you still remember the hype back then when new Harry Potter books were released? Do you think we could experience this level of hype or something similar in the next 50 years?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Pope Francis reveals some secrets — and keeps many others — in new memoir\n",
            "      Añadido con 38 comentarios válidos\n",
            "    Procesando hilo: About the hatred for Holden Caulfield...\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Finished reading Name of the Rose\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: The Parable of the Talents: the best book I never want to read again.\n",
            "      Añadido con 26 comentarios válidos\n",
            "  Extrayendo de books con filtro: month\n",
            "    Procesando hilo: George R.R. Martin says 'The Winds of Winter' is 'the curse of my life'\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Trump administration reportedly moves to ban Jackie Robinson biography from Naval Academy library\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: This is how Facebook won Donald Trump the 2016 election.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Entire staff at federal agency that funds libraries and museums put on leave\n",
            "      Añadido con 50 comentarios válidos\n",
            "Completado: books - 20 hilos guardados en datasets\\books_data_3.0.json\n",
            "\n",
            "Procesando subreddit: travel...\n",
            "  Extrayendo de travel con filtro: all\n",
            "    Procesando hilo: I cycled across Mongolia. Here are some pics. \n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I visited North Korea recently, these are some of the photos.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: i went on a solo trip thinking i’d “find myself” or whatever… but instead i got drunk w a 73yo italian grandma & ended up in a village wedding??\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Taken with a phone out of my hotel window in Venice... and no boats!\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Taking a ride on the Bernina Express through the Alps\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Wife and I hate big social events and love traveling. So rather than a normal wedding, traveled to Switzerland and did our vows in private. Photo from the day\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: The exact moment I took a step too close to the border between North and South Korea and got a push on the butt from two NK soldiers. What's your favorite travel photo of yourself?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Splurged on a hotel in Patagonia\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de travel con filtro: week\n",
            "    Procesando hilo: Tokyo under cherry blossoms, Mar/Apr 2025\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I keep getting stopped at the US border and I just found it’s because of multiple “no shows”?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Four days in Tirana, Albania in April 2025\n",
            "      Añadido con 46 comentarios válidos\n",
            "    Procesando hilo: Istanbul has gone over the edge as an enjoyable vacation destination.  It is legitimately nothing more than a nuisance now\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Is it just me or are airports weird?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: how do people have the means to travel regularly\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Unpopular Opinion: I'd rather sit at home than an airport lounge\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Do border guards worldwide have a secret competition who puts more crooked stamps in most random passport pages?\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de travel con filtro: month\n",
            "    Procesando hilo: First and Maybe Last Visit to India?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Oaxaca, Mexico — If there was ever a city in Mexico that embodies the spirit of magical realism this is it\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: My trip to Antarctica: icebergs, penguins, sleeping on the ice etc.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Passport was taken away when coming home from international flight?\n",
            "      Añadido con 50 comentarios válidos\n",
            "Completado: travel - 20 hilos guardados en datasets\\travel_data_3.0.json\n",
            "\n",
            "Procesando subreddit: gaming...\n",
            "  Extrayendo de gaming con filtro: all\n",
            "    Procesando hilo: Take your time, you got this\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: My parents (late 70s) got me a ps5 controller for Christmas. I do not own a playstation 5...\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I got off the horse by accident right before a cutscene in red dead\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: boy gamer\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Minesweeper 99 x 99, 1500 mines. Took me about 2.5 hours to finish, nerve-wracking. No one might care, but just wanted to share this.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: The perfect cosplay doesn’t ex...\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: 'Play until we lose'\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Can we please boycott Star Wars battlefront 2\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de gaming con filtro: week\n",
            "    Procesando hilo: Not a single NPC has thanked me once.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Bethesda has gifted every member of Skyblivion Team free keys for Oblivion Remastered, following its release earlier today.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I just bought red dead redemption, feeling excited!\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: The Oblivion remaster character creator is just insane.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I’ve defeated the final boss of Elden ring 10,000 times as a summon! My characters name is LetMeSoloThem (let me solo her inspired me, but we are not the same person)\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Maxing sneak in Oblivion the proper way\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Is this what it feels like to play a game for the first time again?(oblivion)\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Traded a few things for a Lenovo legion go today\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de gaming con filtro: month\n",
            "    Procesando hilo: A comparison between the most graphically detailed eyes in gaming\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: My wife thinks video games are juvenile and playing them makes me less attractive.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Game console button layout\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de gaming con filtro: year\n",
            "    Procesando hilo: The Nostalgia\n",
            "      Añadido con 50 comentarios válidos\n",
            "Completado: gaming - 20 hilos guardados en datasets\\gaming_data_3.0.json\n",
            "\n",
            "Procesando subreddit: EnglishLearning...\n",
            "  Extrayendo de EnglishLearning con filtro: all\n",
            "    Procesando hilo: American English vs British English \n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why is there an apostrophe?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Well.....nobody is perfect :)\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: What does it mean?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: What actually means draw in this context?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: English is definitely a weird language.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: What does this mean? \n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why is it singular?\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de EnglishLearning con filtro: week\n",
            "    Procesando hilo: Which one is it?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: what does ‘the fuck out of me’ mean?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: How can I speak respectfully in English without using honorifics like 'Anh', 'Chị', or 'Chú'?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Can someomne explain to me why the To in the frist sentence, please?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why is there a \"to\" in the last sentence? It sounds more natural to me without it\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Do average English natives know this word?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Help me with this question\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why is this question considered ‘awful English’?\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de EnglishLearning con filtro: month\n",
            "    Procesando hilo: It ain't easy\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why can't I say nobody instead of no one?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Hi native speakers, would you say this is a difficult test?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: How do I retain my English level?\n",
            "      Añadido con 50 comentarios válidos\n",
            "Completado: EnglishLearning - 20 hilos guardados en datasets\\EnglishLearning_data_3.0.json\n",
            "\n",
            "Procesando subreddit: Design...\n",
            "  Extrayendo de Design con filtro: all\n",
            "    Procesando hilo: The new cover of TIME\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Take only what you need\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: National Geographic Cover\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: I saw this logo today, I think it's a fantastic design\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: My 5-minute redesign of Hillary's new logo. What do you think?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: What clients really want.\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: 🍊\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Some of my recent obsession with red/orange\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de Design con filtro: week\n",
            "    Procesando hilo: Which one is better?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Losing Income to AI\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Woodstock Inn Brewery using AI on their beer cans\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: We are not a company that needs a designer...\n",
            "      Añadido con 44 comentarios válidos\n",
            "    Procesando hilo: A very simple Halftone generator tool\n",
            "      Descartado: solo 12 comentarios válidos\n",
            "    Procesando hilo: Adobe? Are you really playing f*king videos when I open PhotoShop?! OMFG.\n",
            "      Añadido con 40 comentarios válidos\n",
            "    Procesando hilo: Is this how you vertically alligning numbers?\n",
            "      Añadido con 26 comentarios válidos\n",
            "    Procesando hilo: Can't Find a Job... Am I Being Aged Out?\n",
            "      Añadido con 50 comentarios válidos\n",
            "  Extrayendo de Design con filtro: month\n",
            "    Procesando hilo: Am I the only one who isn’t bothered by this?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why do consumers hate it when brands try to connect with them?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Is this the end?\n",
            "      Añadido con 50 comentarios válidos\n",
            "    Procesando hilo: Why do AI company logos look like buttholes?\n",
            "      Añadido con 41 comentarios válidos\n",
            "    Procesando hilo: Who else wants Disney to bring back 2D animation?\n",
            "      Añadido con 40 comentarios válidos\n",
            "Completado: Design - 20 hilos guardados en datasets\\Design_data_3.0.json\n",
            "\n",
            "Extracción completa.\n"
          ]
        }
      ],
      "source": [
        "# Procesamiento de cada subreddit\n",
        "for subreddit_name in subreddits:\n",
        "    print(f\"Procesando subreddit: {subreddit_name}...\")\n",
        "    \n",
        "    subreddit_data = {\n",
        "        \"subreddit_name\": subreddit_name,\n",
        "        \"threads\": []\n",
        "    }\n",
        "\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    threads_collected = 0  # Contador de hilos extraídos\n",
        "    extracted_thread_ids = set()  # Para evitar duplicados\n",
        "\n",
        "    # Diferentes filtros de tiempo\n",
        "    time_filters = ['all', 'week', 'month', 'year']\n",
        "\n",
        "    for time_filter in time_filters:\n",
        "        if threads_collected >= 20:\n",
        "            break\n",
        "        \n",
        "        print(f\"  Extrayendo de {subreddit_name} con filtro: {time_filter}\")\n",
        "\n",
        "        try:\n",
        "            threads = list(subreddit.top(time_filter=time_filter, limit=8))  # Procesamos 8 hilos para cada temporalidad para mayor rapidez y evitar bloqueos\n",
        "            \n",
        "            for thread in threads:\n",
        "                if threads_collected >= 20 or thread.id in extracted_thread_ids:  # Evitar duplicados\n",
        "                    continue  \n",
        "\n",
        "                print(f\"    Procesando hilo: {thread.title}\")\n",
        "\n",
        "                # Información del hilo\n",
        "                thread_info = {\n",
        "                    \"thread_id\": thread.id,\n",
        "                    \"title\": thread.title,\n",
        "                    \"author\": str(thread.author),\n",
        "                    \"created_utc\": datetime.utcfromtimestamp(thread.created_utc).isoformat(),\n",
        "                    \"score\": thread.score,\n",
        "                    \"url\": thread.url,\n",
        "                    \"permalink\": thread.permalink,\n",
        "                    \"time_filter\": time_filter,\n",
        "                    \"subreddit_name\": subreddit_name,\n",
        "                    \"comments\": []\n",
        "                }\n",
        "\n",
        "\n",
        "                # Extraer comentarios\n",
        "                try:\n",
        "                    thread.comments.replace_more(limit=3)  # Evita cargar demasiados comentarios\n",
        "                    flat_comments = thread.comments.list()\n",
        "                except Exception as e:\n",
        "                    print(f\"      Error al cargar comentarios: {e}\")\n",
        "                    continue\n",
        "\n",
        "                valid_comments_count = 0\n",
        "\n",
        "                for comment in flat_comments:\n",
        "                    if valid_comments_count >= 50:\n",
        "                        break\n",
        "\n",
        "                    if is_valid_comment(comment.body): # Si el comentario pasa el filtro de validez es almacenado en la lista de comentarios de su respectivo hilo\n",
        "\n",
        "                        comment_info = {\n",
        "                            'comment_id': comment.id,\n",
        "                            'author': str(comment.author),\n",
        "                            'text': comment.body,\n",
        "                            'score': comment.score,\n",
        "                            'created_utc': datetime.utcfromtimestamp(comment.created_utc).isoformat(),\n",
        "                            'subreddit_name': subreddit_name,\n",
        "                            'thread_id': thread.id,\n",
        "                            'is_submitter': comment.is_submitter,\n",
        "                            'permalink': comment.permalink\n",
        "                        }\n",
        "\n",
        "                        thread_info[\"comments\"].append(comment_info)\n",
        "                        valid_comments_count += 1\n",
        "\n",
        "                if valid_comments_count >= 25:  # Mínimo de 25 comentarios válidos, sino, el hilo es descartado\n",
        "                    subreddit_data[\"threads\"].append(thread_info)\n",
        "                    extracted_thread_ids.add(thread.id)\n",
        "                    threads_collected += 1\n",
        "                    print(f\"      Añadido con {valid_comments_count} comentarios válidos\")\n",
        "                else:\n",
        "                    print(f\"      Descartado: solo {valid_comments_count} comentarios válidos\")\n",
        "\n",
        "                time.sleep(1.5)  # Evitar exceso de peticiones a Reddit, nos permite cumplir con las normas de uso de la API y evitar bloqueos temporales\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {subreddit_name} con filtro {time_filter}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Guardar en JSON\n",
        "    os.makedirs('datasets', exist_ok=True)\n",
        "    output_file = os.path.join('datasets', f\"{subreddit_name}_data_3.0.json\")\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(subreddit_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Completado: {subreddit_name} - {threads_collected} hilos guardados en {output_file}\\n\")\n",
        "\n",
        "print(\"Extracción completa.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PROCESAMIENTO LÉXICO Y CREACIÓN DE UN DATAFRAME PARA TRATAR CON MAYOR FACILIDAD LOS COMENTARIOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "import spacy\n",
        "\n",
        "# Cargamos el modelo de spaCy para realizar la Lematización\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Configuración de SymSpell para realizar la corrección automática de errores ortográficos\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = \"datasets/frequency_dictionary_en_82_765.txt\"  \n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1, separator=\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    # Patrón para detectar emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # símbolos & pictogramas\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # símbolos de transporte & mapas\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FUNCIÓN DE PROCESAMIENTO LÉXICO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, autocorrect=True):\n",
        "    # Eliminar emojis\n",
        "    text = remove_emojis(text)\n",
        "    \n",
        "    # Convertimos a minúsculas\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Procesamiento con spaCy\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Tokenización y eliminación de signos de puntuación y números\n",
        "    tokens = [token.text for token in doc if not token.is_punct and not token.like_num]\n",
        "    \n",
        "    # Corrección ortográfica con SymSpellPy\n",
        "    if autocorrect:\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            suggestions = sym_spell.lookup(token, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "            # Si hay sugerencias, tomamos la primera\n",
        "            # Si no, mantenemos el token original\n",
        "            if suggestions:\n",
        "                corrected_tokens.append(suggestions[0].term)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        tokens = corrected_tokens\n",
        "    \n",
        "    # Eliminación de stopwords\n",
        "    tokens = [token for token in tokens if not token in nlp.Defaults.stop_words]\n",
        "    \n",
        "    # Lematización con spaCy\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "    \n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Procesamiento Léxico con spaCy y SymSpellPy\n",
        "\n",
        "Usamos **spaCy** para el procesamiento completo del texto. Aprovechamos sus capacidades integradas para:\n",
        "\n",
        "- Tokenización  \n",
        "- Detección de números y puntuación  \n",
        "- Uso de su propia lista de *stopwords*  \n",
        "- Lematización más precisa  \n",
        "\n",
        "Además, realizamos la **corrección ortográfica** con **SymSpellPy**.\n",
        "\n",
        "### Ventajas de usar spaCy\n",
        "\n",
        "El proceso es más eficiente porque spaCy está optimizado para procesamiento de texto. La lematización con spaCy suele ser más precisa que con NLTK porque:\n",
        "\n",
        "- Considera el contexto de la palabra  \n",
        "- Tiene mejor manejo de formas irregulares  \n",
        "- Está más actualizado con el lenguaje moderno  \n",
        "- Es más rápido en el procesamiento  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**REALIZACIÓN DEL PROCESAMIENTO Y PREPARACIÓN DE LOS COMENTARIOS PARA SU POSTERIOR USO EN DIVERSAS TAREAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargamos los datos extraídos y procesamos los comentarios\n",
        "all_comments = []\n",
        "dataset_path = 'datasets'\n",
        "\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith('_data_3.0.json'):\n",
        "        with open(os.path.join(dataset_path, filename), 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "            \n",
        "            for thread in data['threads']:\n",
        "                for comment in thread['comments']:\n",
        "                    comment_data = {\n",
        "                        'body': comment['text'],\n",
        "                        'score': comment['score'],\n",
        "                        'created_utc': comment['created_utc'],\n",
        "                        'thread_title': thread['title'],\n",
        "                        'subreddit': data['subreddit_name'],\n",
        "                        'processed_text': preprocess_text(comment['text'], autocorrect=True),\n",
        "                        'processed_text_no_correction': preprocess_text(comment['text'], autocorrect=False)\n",
        "                    }\n",
        "                    all_comments.append(comment_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CREACIÓN DE UN DATAFRAME CON TODOS LOS COMENTARIOS EXTRAÍDOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5866\n"
          ]
        }
      ],
      "source": [
        "print(len(all_comments))  # Obtención de alrededor de 6000 comentarios válidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_text_no_correction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So if you haven't already, there's a bot you c...</td>\n",
              "      <td>2472</td>\n",
              "      <td>2017-11-22T01:50:57</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aussie here. Please fight for this. If the US ...</td>\n",
              "      <td>697</td>\n",
              "      <td>2017-11-22T03:19:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do we have to do this every few months? 😞</td>\n",
              "      <td>810</td>\n",
              "      <td>2017-11-22T03:17:01</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>month</td>\n",
              "      <td>month</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>💰 The 💰 intent 💰 is 💰 to 💰 provide 💰 consumers...</td>\n",
              "      <td>871</td>\n",
              "      <td>2017-11-22T02:21:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>intent        provide    consumer        ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Someone suggested that we flood Trumps twitter...</td>\n",
              "      <td>124</td>\n",
              "      <td>2017-11-22T04:41:47</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>suggest flood trump twitter tweet save net neu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Fucking same shit every month. How many times ...</td>\n",
              "      <td>72</td>\n",
              "      <td>2017-11-22T03:25:33</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>fucking shit month time go try pass shit</td>\n",
              "      <td>fucking shit month time go try pass shit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>We've only done 266k calls... how? We have mor...</td>\n",
              "      <td>261</td>\n",
              "      <td>2017-11-22T03:13:33</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>ave 266k call editor upcoming caller literally</td>\n",
              "      <td>266k call reditor upvote caller literally</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>“The intent is to provide customers with a sen...</td>\n",
              "      <td>118</td>\n",
              "      <td>2017-11-22T03:29:12</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide customer sense accomplishment u...</td>\n",
              "      <td>intent provide customer sense accomplishment u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[To learn about Net Neutrality, why it's impor...</td>\n",
              "      <td>99</td>\n",
              "      <td>2017-11-22T03:13:49</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>learn net neutrality important indoor want too...</td>\n",
              "      <td>learn net neutrality important and/or want too...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Well y'all, I sat through all the prompts and ...</td>\n",
              "      <td>24</td>\n",
              "      <td>2017-11-22T03:36:15</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>sit prompt leave message senator representativ...</td>\n",
              "      <td>y ' sit prompt leave message senator represent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body  score  \\\n",
              "0  So if you haven't already, there's a bot you c...   2472   \n",
              "1  Aussie here. Please fight for this. If the US ...    697   \n",
              "2      Why do we have to do this every few months? 😞    810   \n",
              "3  💰 The 💰 intent 💰 is 💰 to 💰 provide 💰 consumers...    871   \n",
              "4  Someone suggested that we flood Trumps twitter...    124   \n",
              "5  Fucking same shit every month. How many times ...     72   \n",
              "6  We've only done 266k calls... how? We have mor...    261   \n",
              "7  “The intent is to provide customers with a sen...    118   \n",
              "8  [To learn about Net Neutrality, why it's impor...     99   \n",
              "9  Well y'all, I sat through all the prompts and ...     24   \n",
              "\n",
              "           created_utc                                       thread_title  \\\n",
              "0  2017-11-22T01:50:57  Join the Battle for Net Neutrality!! We need t...   \n",
              "1  2017-11-22T03:19:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "2  2017-11-22T03:17:01  Join the Battle for Net Neutrality!! We need t...   \n",
              "3  2017-11-22T02:21:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "4  2017-11-22T04:41:47  Join the Battle for Net Neutrality!! We need t...   \n",
              "5  2017-11-22T03:25:33  Join the Battle for Net Neutrality!! We need t...   \n",
              "6  2017-11-22T03:13:33  Join the Battle for Net Neutrality!! We need t...   \n",
              "7  2017-11-22T03:29:12  Join the Battle for Net Neutrality!! We need t...   \n",
              "8  2017-11-22T03:13:49  Join the Battle for Net Neutrality!! We need t...   \n",
              "9  2017-11-22T03:36:15  Join the Battle for Net Neutrality!! We need t...   \n",
              "\n",
              "  subreddit                                     processed_text  \\\n",
              "0     books  bot text help write email fax free charge sena...   \n",
              "1     books          aussie fight lose net neutrality overflow   \n",
              "2     books                                              month   \n",
              "3     books  intent provide consumer high quality service s...   \n",
              "4     books  suggest flood trump twitter sweet save net neu...   \n",
              "5     books           fucking shit month time go try pass shit   \n",
              "6     books     ave 266k call editor upcoming caller literally   \n",
              "7     books  intent provide customer sense accomplishment u...   \n",
              "8     books  learn net neutrality important indoor want too...   \n",
              "9     books  sit prompt leave message senator representativ...   \n",
              "\n",
              "                        processed_text_no_correction  \n",
              "0  bot text help write email fax free charge sena...  \n",
              "1          aussie fight lose net neutrality overflow  \n",
              "2                                              month  \n",
              "3       intent        provide    consumer        ...  \n",
              "4  suggest flood trump twitter tweet save net neu...  \n",
              "5           fucking shit month time go try pass shit  \n",
              "6          266k call reditor upvote caller literally  \n",
              "7  intent provide customer sense accomplishment u...  \n",
              "8  learn net neutrality important and/or want too...  \n",
              "9  y ' sit prompt leave message senator represent...  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Crear DataFrame\n",
        "df_comments = pd.DataFrame(all_comments)\n",
        "\n",
        "# Mostrar las primeras filas y información del DataFrame\n",
        "df_comments.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_comments.to_csv('datasets/comments_data.csv', index=False, header=True, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-100\")  # También hay 50, 200 o 300 dimensiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_text</th>\n",
              "      <th>embedding_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>[-0.11024348, 0.13865672, 0.24223563, -0.19040...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>[0.27225667, 0.19225435, 0.43482652, -0.350442...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>month</td>\n",
              "      <td>[0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>[-0.05243463, 0.061616946, 0.26537868, 0.02395...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>[-0.05058644, -0.0106403595, 0.18712802, -0.19...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      processed_text  \\\n",
              "0  bot text help write email fax free charge sena...   \n",
              "1          aussie fight lose net neutrality overflow   \n",
              "2                                              month   \n",
              "3  intent provide consumer high quality service s...   \n",
              "4  suggest flood trump twitter sweet save net neu...   \n",
              "\n",
              "                                      embedding_mean  \n",
              "0  [-0.11024348, 0.13865672, 0.24223563, -0.19040...  \n",
              "1  [0.27225667, 0.19225435, 0.43482652, -0.350442...  \n",
              "2  [0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...  \n",
              "3  [-0.05243463, 0.061616946, 0.26537868, 0.02395...  \n",
              "4  [-0.05058644, -0.0106403595, 0.18712802, -0.19...  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Función para obtener el promedio de embeddings de las palabras del texto\n",
        "def get_mean_embedding(text):\n",
        "    vectors = [model[word] for word in text.split() if word in model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.nan  # o np.zeros(50) si prefieres un vector nulo\n",
        "\n",
        "# Aplicamos la función a la columna del DataFrame\n",
        "df_comments[\"embedding_mean\"] = df_comments[\"processed_text\"].apply(get_mean_embedding)\n",
        "\n",
        "# Mostramos los primeros resultados\n",
        "df_comments[[\"processed_text\", \"embedding_mean\"]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>score</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>thread_title</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_text_no_correction</th>\n",
              "      <th>embedding_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So if you haven't already, there's a bot you c...</td>\n",
              "      <td>2472</td>\n",
              "      <td>2017-11-22T01:50:57</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>bot text help write email fax free charge sena...</td>\n",
              "      <td>[-0.11024348, 0.13865672, 0.24223563, -0.19040...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aussie here. Please fight for this. If the US ...</td>\n",
              "      <td>697</td>\n",
              "      <td>2017-11-22T03:19:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>aussie fight lose net neutrality overflow</td>\n",
              "      <td>[0.27225667, 0.19225435, 0.43482652, -0.350442...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why do we have to do this every few months? 😞</td>\n",
              "      <td>810</td>\n",
              "      <td>2017-11-22T03:17:01</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>month</td>\n",
              "      <td>month</td>\n",
              "      <td>[0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>💰 The 💰 intent 💰 is 💰 to 💰 provide 💰 consumers...</td>\n",
              "      <td>871</td>\n",
              "      <td>2017-11-22T02:21:54</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>intent provide consumer high quality service s...</td>\n",
              "      <td>intent        provide    consumer        ...</td>\n",
              "      <td>[-0.05243463, 0.061616946, 0.26537868, 0.02395...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Someone suggested that we flood Trumps twitter...</td>\n",
              "      <td>124</td>\n",
              "      <td>2017-11-22T04:41:47</td>\n",
              "      <td>Join the Battle for Net Neutrality!! We need t...</td>\n",
              "      <td>books</td>\n",
              "      <td>suggest flood trump twitter sweet save net neu...</td>\n",
              "      <td>suggest flood trump twitter tweet save net neu...</td>\n",
              "      <td>[-0.05058644, -0.0106403595, 0.18712802, -0.19...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body  score  \\\n",
              "0  So if you haven't already, there's a bot you c...   2472   \n",
              "1  Aussie here. Please fight for this. If the US ...    697   \n",
              "2      Why do we have to do this every few months? 😞    810   \n",
              "3  💰 The 💰 intent 💰 is 💰 to 💰 provide 💰 consumers...    871   \n",
              "4  Someone suggested that we flood Trumps twitter...    124   \n",
              "\n",
              "           created_utc                                       thread_title  \\\n",
              "0  2017-11-22T01:50:57  Join the Battle for Net Neutrality!! We need t...   \n",
              "1  2017-11-22T03:19:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "2  2017-11-22T03:17:01  Join the Battle for Net Neutrality!! We need t...   \n",
              "3  2017-11-22T02:21:54  Join the Battle for Net Neutrality!! We need t...   \n",
              "4  2017-11-22T04:41:47  Join the Battle for Net Neutrality!! We need t...   \n",
              "\n",
              "  subreddit                                     processed_text  \\\n",
              "0     books  bot text help write email fax free charge sena...   \n",
              "1     books          aussie fight lose net neutrality overflow   \n",
              "2     books                                              month   \n",
              "3     books  intent provide consumer high quality service s...   \n",
              "4     books  suggest flood trump twitter sweet save net neu...   \n",
              "\n",
              "                        processed_text_no_correction  \\\n",
              "0  bot text help write email fax free charge sena...   \n",
              "1          aussie fight lose net neutrality overflow   \n",
              "2                                              month   \n",
              "3       intent        provide    consumer        ...   \n",
              "4  suggest flood trump twitter tweet save net neu...   \n",
              "\n",
              "                                      embedding_mean  \n",
              "0  [-0.11024348, 0.13865672, 0.24223563, -0.19040...  \n",
              "1  [0.27225667, 0.19225435, 0.43482652, -0.350442...  \n",
              "2  [0.21605, 0.14269, 0.21949, -0.11729, 0.28479,...  \n",
              "3  [-0.05243463, 0.061616946, 0.26537868, 0.02395...  \n",
              "4  [-0.05058644, -0.0106403595, 0.18712802, -0.19...  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_comments.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Design       0.62      0.53      0.57       188\n",
            "      Economics       0.69      0.83      0.76       193\n",
            "EnglishLearning       0.76      0.65      0.70       198\n",
            "          books       0.60      0.54      0.57       193\n",
            "         gaming       0.58      0.62      0.60       200\n",
            "         travel       0.63      0.68      0.65       199\n",
            "\n",
            "       accuracy                           0.64      1171\n",
            "      macro avg       0.65      0.64      0.64      1171\n",
            "   weighted avg       0.65      0.64      0.64      1171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convertir las listas de 'embedding_padded' en una matriz de NumPy\n",
        "X = np.vstack(df_comments[\"embedding_mean\"].dropna().values)\n",
        "\n",
        "# Etiquetas\n",
        "y = df_comments.loc[df_comments[\"embedding_mean\"].notna(), \"subreddit\"] # Asegúrate de que las etiquetas estén alineadas con X\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "# Crear el clasificador Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "# Entrenar el modelo\n",
        "rf.fit(X_train, y_train)\n",
        "# Hacer predicciones\n",
        "y_pred = rf.predict(X_test)\n",
        "# Evaluar el modelo\n",
        "print(classification_report(y_test, y_pred, zero_division=0))  # Añadido zero_division=0 para evitar errores en caso de división por cero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n",
            "Mejores parámetros encontrados: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "\n",
            "Reporte en test set:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Design       0.61      0.64      0.62       188\n",
            "      Economics       0.72      0.85      0.78       193\n",
            "EnglishLearning       0.76      0.71      0.73       198\n",
            "          books       0.68      0.60      0.63       193\n",
            "         gaming       0.67      0.63      0.65       200\n",
            "         travel       0.70      0.72      0.71       199\n",
            "\n",
            "       accuracy                           0.69      1171\n",
            "      macro avg       0.69      0.69      0.69      1171\n",
            "   weighted avg       0.69      0.69      0.69      1171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['linear'],\n",
        "        'C': [0.1, 1, 10]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['rbf'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': ['scale', 0.01, 0.1, 1]\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'degree': [2, 3, 4],\n",
        "        'gamma': ['scale', 0.01, 0.1]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Crear el GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    SVC(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs=-1  # Usa todos los núcleos disponibles\n",
        ")\n",
        "\n",
        "# Ajustar al set de entrenamiento\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mostrar los mejores parámetros\n",
        "print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluar el mejor modelo en el set de prueba\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nReporte en test set:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM7WT3JKuGij/JurAfV1xur",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mi_entorno",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
